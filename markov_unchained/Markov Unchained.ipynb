{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains\n",
    "**Using Probability to Generate Text**\n",
    "\n",
    "By Jade Tabony\n",
    "\n",
    "\n",
    "### What are Markov Chains?\n",
    "Markov chains describe a stochastic process that is used to predict a series of events, or a chain, if you will.  The occurence of each event is dependent only on the current state of the system.  Because of this, Markov chains are said to be \"Memoryless\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So what is happening?\n",
    "\n",
    "![chain](img/two-cities.jpg)\n",
    "http://oatzy.blogspot.com/2011/04/markovs-next-tweet.html\n",
    "\n",
    "When applied to building sentences, this means that as the system is constructing the sentence, word by word, the next word is decided only on the word preceding it, having nothing to do with the rest of the sentence.  This can lead to some really interesting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "- https://twitter.com/captain_markov\n",
    "- https://twitter.com/markovchainme\n",
    "- https://twitter.com/scifri_ebooks\n",
    "- https://twitter.com/Pliny_theElder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xkcd](img/xkcd.png)\n",
    "https://xkcd.com/1068/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets build one!!\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "### Lewis Carroll goes for a Hike.\n",
    "\n",
    "![chesirecat](img/chesire_cat.png)\n",
    "\n",
    "For my example, I'm going to train my chain on Lewis Carroll books and hike descriptions from the Washington Trails Association.\n",
    "\n",
    "First off, import your necessary libraries.  If you do not have anaconda installed, you may have to pip install some of the libraries.  Pandas was necessary for my example, since my hike data was stored in a csv file, but you may not need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to need to text to \"train\" your model on.  My text was downloaded from Project Gutenberg and scraped from the WTA website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_text = open('data/AliceInWonderland.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hikes = pd.read_csv('data/washington_hikes_clean_noout.csv')\n",
    "hikes = hikes[['hike_name', 'hike_desc']].dropna()\n",
    "hikes['desc_len'] = [len(text) for text in hikes['hike_desc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to build the code for our Markov Chain bot.  I've decided to build it with the potential for both bigram and single word seeds. Some of this code was borrowed from [this blog post](https://sookocheff.com/post/nlp/ngram-modeling-with-markov-chains/), which I customized to include the potential for training on both bigrams and trigrams, and to include the tweeting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MarkovChain(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory = {}\n",
    "        self.bigram_memory = {}\n",
    "\n",
    "    def _learn_key(self, key, value):\n",
    "        if key not in self.memory:\n",
    "            self.memory[key] = []\n",
    "\n",
    "        self.memory[key].append(value)\n",
    "    \n",
    "    def _learn_bigramkey(self, key, value):\n",
    "        if key not in self.memory:\n",
    "            self.bigram_memory[key] = []\n",
    "\n",
    "        self.bigram_memory[key].append(value)\n",
    "\n",
    "    def learn(self, text, bigram_keys=True):\n",
    "        #tokens = [item for item in map(string.strip, re.split(\"(\\W+)\", text)) if len(item) > 0]\n",
    "        tokens = text.split()\n",
    "        if bigram_keys: \n",
    "            trigrams = [(tokens[i], tokens[i + 1], tokens[i + 2]) for i in range(0, len(tokens) - 2)]\n",
    "            for trigram in trigrams:\n",
    "                self._learn_bigramkey((trigram[0], trigram[1]), trigram[2])\n",
    "        else: \n",
    "            bigrams = [(tokens[i], tokens[i + 1]) for i in range(0, len(tokens) - 1)]\n",
    "            for bigram in bigrams:\n",
    "                self._learn_key(bigram[0], bigram[1])\n",
    "\n",
    "    def _next(self, current_state):\n",
    "        next_possible = self.memory.get(current_state)\n",
    "\n",
    "        if not next_possible:\n",
    "            next_possible = self.memory.keys()\n",
    "\n",
    "        return random.sample(next_possible, 1)[0]\n",
    "    \n",
    "    def _next_bigram(self, current_state):\n",
    "        next_possible = self.bigram_memory.get(current_state)\n",
    "\n",
    "        if not next_possible:\n",
    "            next_possible = self.bigram_memory.keys()\n",
    "\n",
    "        return random.sample(next_possible, 1)[0]\n",
    "    \n",
    "    def tweet(self, min_length=70, max_length=140, use_bigram=True):\n",
    "        tweet_length = random.randint(min_length, max_length)\n",
    "        tweet = []\n",
    "        if use_bigram:\n",
    "            seed = random.choice(self.bigram_memory.keys())\n",
    "            tweet.append(seed[0])\n",
    "            tweet.append(seed[1])\n",
    "            tweet_length -= len(seed) \n",
    "            while tweet_length>0:\n",
    "                next_word = self._next_bigram((tweet[-2], tweet[-1]))\n",
    "                tweet.append(next_word)\n",
    "                tweet_length-=len(next_word)\n",
    "        else: \n",
    "            seed = random.choice(self.memory.keys())\n",
    "            tweet.append(seed)\n",
    "            tweet_length -= len(seed) \n",
    "            while tweet_length>0:\n",
    "                next_word = self._next(tweet[-1])\n",
    "                tweet.append(next_word)\n",
    "                tweet_length-=len(next_word)\n",
    "        tweet[0] = tweet[0].capitalize()\n",
    "        tweet[-1] = tweet[-1].strip(string.punctuation) + random.choice(['.', '!', '?'])\n",
    "        return \" \".join(tweet)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your Markov Chain bot class built, you can initialize, train and tweet with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lewis = MarkovChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lewis.learn(alice_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over. Alice was too much frightened to say a word, but slowly followed her back to the White Rabbit read out the verses!'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lewis.tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add some hike descriptions!  These descriptions were scraped from the WTA website using requests and BeautifulSoup for another project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for hike in hikes['hike_desc']:\n",
    "    lewis.learn(hike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gain 400 feet below, while far beyond the rich foliage and wildlife habitat of chukar--a beautiful upland game bird that prefers steep slopes and arid landscapes.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lewis.tweet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quickly encounter the Hidden Forest Trail uphill, at first climbing steeply then leveling out. After 0.2 miles across a grassy rock slab?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lewis.tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... Now most of my tweets seem to be dominated by WTA text. Let's check how long each body of text is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53024"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alice_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2596937"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hikes.desc_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, most of my memory consists of WTA text.  So I'll add more Lewis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162063"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "looking_glass = open('data/ThroughTheLookingGlass.txt', 'r').read()\n",
    "len(looking_glass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lewis.learn(looking_glass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And second, a rocky outcrop.In summer, as you can, And sprinkle the table till she had found the Red King, Kitty?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lewis.tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BETTER!!!**\n",
    "\n",
    "![grumpy-cat](img/grumpy_cat.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things you might consider:\n",
    "- Training your Markov bot to start in a logical spot. (But logic is boring)\n",
    "- Similarly, find a logical ending. (see previous point)\n",
    "- If you're training using multiple bodies of text of significantly differing length, consider under or oversampling.\n",
    "- Ensuring that opened quotes or parentheses close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markovify \n",
    "#### Just in case you don't want to write this from scratch\n",
    "\n",
    "\n",
    "https://github.com/jsvine/markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Alice's great surprise, the Duchess's arm that was sitting on the back.\n",
      "This did not venture to go down the chimney as she was nine feet high, and she felt certain it must be really offended.\n",
      "He came in sight of the house and found that she was shrinking rapidly; so she set to work at once to eat the comfits; this caused some noise and confusion, as the Rabbit and had to kneel down on the bank, and of having nothing to do.\n",
      "The judge, by the White Rabbit, with a little shriek and a large dish of tarts upon it.\n",
      "She looked up, but it was the King and he wore his crown over his great wig.\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "with open(\"data/AliceInWonderland.txt\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "    \n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MarkovBot!\n",
    "Just in case you want to do even less\n",
    "\n",
    "http://www.pygaze.org/2016/03/how-to-code-twitter-bot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESOURCES\n",
    "\n",
    "- [Project Gutenberg](https://www.gutenberg.org/)\n",
    "- [The Washington Trails Association](https://wta.org)\n",
    "- https://sookocheff.com/post/nlp/ngram-modeling-with-markov-chains/\n",
    "- http://www.onthelambda.com/2014/02/20/how-to-fake-a-sophisticated-knowledge-of-wine-with-markov-chains/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
