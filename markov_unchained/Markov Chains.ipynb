{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains\n",
    "**Using Probability to Generate Text**\n",
    "\n",
    "By Jade Tabony\n",
    "\n",
    "\n",
    "### What are Markov Chains?\n",
    "Markov chains describe a stochastic process that is used to predict a series of events, or a chain, if you will.  The occurence of each event is dependent only on the current state of the system.  Because of this, Markov chains are said to be \"Memoryless\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So what is happening?\n",
    "\n",
    "![chain](img/two-cities.jpg)\n",
    "http://oatzy.blogspot.com/2011/04/markovs-next-tweet.html\n",
    "\n",
    "When applied to building sentences, this means that as the system is constructing the sentence, word by word, the next word is decided only on the word preceding it, having nothing to do with the rest of the sentence.  This can lead to some really interesting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "- https://twitter.com/captain_markov\n",
    "- https://twitter.com/markovchainme\n",
    "- https://twitter.com/scifri_ebooks\n",
    "- https://twitter.com/Pliny_theElder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xkcd](img/xkcd.png)\n",
    "https://xkcd.com/1068/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets build one!!\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "### Lewis Carroll goes for a Hike.\n",
    "\n",
    "![chesirecat](img/chesire_cat.png)\n",
    "\n",
    "For my example, I'm going to train my chain on Lewis Carroll books and hike descriptions from the Washington Trails Association.\n",
    "\n",
    "First off, import your necessary libraries.  If you do not have anaconda installed, you may have to pip install some of the libraries.  Pandas was necessary for my example, since my hike data was stored in a csv file, but you may not need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to need to text to \"train\" your model on.  My text was downloaded from Project Gutenberg and scraped from the WTA website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice_text = open('data/AliceInWonderland.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hikes = pd.read_csv('data/washington_hikes_clean_noout.csv')\n",
    "hikes = hikes[['hike_name', 'hike_desc']].dropna()\n",
    "hikes['desc_len'] = [len(text) for text in hikes['hike_desc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to build the code for our Markov Chain bot.  I've decided to build it with the potential for both bigram and single word seeds. Some of this code was borrowed from [this blog post](https://sookocheff.com/post/nlp/ngram-modeling-with-markov-chains/), which I customized to include the potential for training on both bigrams and trigrams, and to include the tweeting functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MarkovChain(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.memory = {}\n",
    "        self.bigram_memory = {}\n",
    "\n",
    "    def _learn_key(self, key, value):\n",
    "        if key not in self.memory:\n",
    "            self.memory[key] = []\n",
    "\n",
    "        self.memory[key].append(value)\n",
    "    \n",
    "    def _learn_bigramkey(self, key, value):\n",
    "        if key not in self.memory:\n",
    "            self.bigram_memory[key] = []\n",
    "\n",
    "        self.bigram_memory[key].append(value)\n",
    "\n",
    "    def learn(self, text, bigram_keys=True):\n",
    "        #tokens = [item for item in map(string.strip, re.split(\"(\\W+)\", text)) if len(item) > 0]\n",
    "        tokens = text.split()\n",
    "        if bigram_keys: \n",
    "            trigrams = [(tokens[i], tokens[i + 1], tokens[i + 2]) for i in range(0, len(tokens) - 2)]\n",
    "            for trigram in trigrams:\n",
    "                self._learn_bigramkey((trigram[0], trigram[1]), trigram[2])\n",
    "        else: \n",
    "            bigrams = [(tokens[i], tokens[i + 1]) for i in range(0, len(tokens) - 1)]\n",
    "            for bigram in bigrams:\n",
    "                self._learn_key(bigram[0], bigram[1])\n",
    "\n",
    "    def _next(self, current_state):\n",
    "        next_possible = self.memory.get(current_state)\n",
    "\n",
    "        if not next_possible:\n",
    "            next_possible = self.memory.keys()\n",
    "\n",
    "        return random.sample(next_possible, 1)[0]\n",
    "    \n",
    "    def _next_bigram(self, current_state):\n",
    "        next_possible = self.bigram_memory.get(current_state)\n",
    "\n",
    "        if not next_possible:\n",
    "            next_possible = self.bigram_memory.keys()\n",
    "\n",
    "        return random.sample(next_possible, 1)[0]\n",
    "    \n",
    "    def tweet(self, min_length=70, max_length=140, use_bigram=True):\n",
    "        tweet_length = random.randint(min_length, max_length)\n",
    "        tweet = []\n",
    "        if use_bigram:\n",
    "            seed = random.choice(self.bigram_memory.keys())\n",
    "            tweet.append(seed[0])\n",
    "            tweet.append(seed[1])\n",
    "            tweet_length -= len(seed) \n",
    "            while tweet_length>0:\n",
    "                next_word = self._next_bigram((tweet[-2], tweet[-1]))\n",
    "                tweet.append(next_word)\n",
    "                tweet_length-=len(next_word)\n",
    "        else: \n",
    "            seed = random.choice(self.memory.keys())\n",
    "            tweet.append(seed)\n",
    "            tweet_length -= len(seed) \n",
    "            while tweet_length>0:\n",
    "                next_word = self._next(tweet[-1])\n",
    "                tweet.append(next_word)\n",
    "                tweet_length-=len(next_word)\n",
    "        tweet[0] = tweet[0].capitalize()\n",
    "        tweet[-1] = tweet[-1].strip(string.punctuation) + random.choice(['.', '!', '?'])\n",
    "        return \" \".join(tweet)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your Markov Chain bot class built, you can initialize, train and tweet with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lewis = MarkovChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lewis.learn(alice_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lewis.tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add some hike descriptions!  These descriptions were scraped from the WTA website using requests and BeautifulSoup for another project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for hike in hikes['hike_desc']:\n",
    "    lewis.learn(hike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lewis.tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... Now most of my tweets seem to be dominated by WTA text. Let's check how long each body of text is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(alice_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(hikes.desc_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, most of my memory consists of WTA text.  So I'll add more Lewis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "looking_glass = open('data/ThroughTheLookingGlass.txt', 'r').read()\n",
    "len(looking_glass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lewis.learn(looking_glass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lewis.tweet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BETTER!!!**\n",
    "\n",
    "![grumpy-cat](img/grumpy_cat.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things you might consider:\n",
    "- Training your Markov bot to start in a logical spot. (But logic is boring)\n",
    "- Similarly, find a logical ending. (see previous point)\n",
    "- If you're training using multiple bodies of text of significantly differing length, consider under or oversampling.\n",
    "- Ensuring that opened quotes or parentheses close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESOURCES\n",
    "\n",
    "- [Project Gutenberg](https://www.gutenberg.org/)\n",
    "- [The Washington Trails Association](https://wta.org)\n",
    "- https://sookocheff.com/post/nlp/ngram-modeling-with-markov-chains/\n",
    "- http://www.onthelambda.com/2014/02/20/how-to-fake-a-sophisticated-knowledge-of-wine-with-markov-chains/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
